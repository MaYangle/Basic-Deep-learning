{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaYangle/Basic-Deep-learning/blob/main/text_generation_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovpZyIhNIgoq"
      },
      "source": [
        "# Text generation with an RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ak0hXaLBg-rd"
      },
      "source": [
        "### Learning Objectives\n",
        "\n",
        "- Learn how to generate text using a RNN\n",
        "- Create training examples and targets for text generation\n",
        "- Build a RNN model for sequence generation using Keras Subclassing\n",
        "- Create a text generator and evaluate the output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwpJ5IffzRG6"
      },
      "source": [
        "This tutorial demonstrates how to generate text using a character-based RNN. You will work with a dataset of Shakespeare's writing from Andrej Karpathy's [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). Given a sequence of characters from this data (\"Shakespear\"), train a model to predict the next character in the sequence (\"e\"). Longer sequences of text can be generated by calling the model repeatedly.\n",
        "\n",
        "Below is the sample output when the model in this tutorial trained for 30 epochs, and started with the prompt \"Q\":"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcygKkEVZBaa"
      },
      "source": [
        "<pre>\n",
        "QUEENE:\n",
        "I had thought thou hadst a Roman; for the oracle,\n",
        "Thus by All bids the man against the word,\n",
        "Which are so weak of care, by old care done;\n",
        "Your children were in your holy love,\n",
        "And the precipitation through the bleeding throne.\n",
        "\n",
        "BISHOP OF ELY:\n",
        "Marry, and will, my lord, to weep in such a one were prettiest;\n",
        "Yet now I was adopted heir\n",
        "Of the world's lamentable day,\n",
        "To watch the next way with his father with his face?\n",
        "\n",
        "ESCALUS:\n",
        "The cause why then we are all resolved more sons.\n",
        "\n",
        "VOLUMNIA:\n",
        "O, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, it is no sin it should be dead,\n",
        "And love and pale as any will to that word.\n",
        "\n",
        "QUEEN ELIZABETH:\n",
        "But how long have I heard the soul for this world,\n",
        "And show his hands of life be proved to stand.\n",
        "\n",
        "PETRUCHIO:\n",
        "I say he look'd on, if I must be content\n",
        "To stay him from the fatal of our country's bliss.\n",
        "His lordship pluck'd from this sentence then for prey,\n",
        "And then let us twain, being the moon,\n",
        "were she such a case as fills m\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bGsCP9DZFQ5"
      },
      "source": [
        "While some of the sentences are grammatical, most do not make sense. The model has not learned the meaning of words, but here are some things to consider:\n",
        "\n",
        "* The model is character-based. When training started, the model did not know how to spell an English word, or that words were even a unit of text.\n",
        "\n",
        "* The structure of the output resembles a play—blocks of text generally begin with a speaker name, in all capital letters similar to the dataset.\n",
        "\n",
        "* As demonstrated below, the model is trained on small batches of text (100 characters each), and is still able to generate a longer sequence of text with coherent structure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srXC6pLGLwS6"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGyKZj3bzf9p"
      },
      "source": [
        "### Import TensorFlow and other libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ozNSeFN8g-rl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yG_n40gFzf9s",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHDoRoc5PKWz"
      },
      "source": [
        "### Download the Shakespeare dataset\n",
        "\n",
        "Change the following line to run this code on your own data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHjdCjDuSvX_"
      },
      "source": [
        "### Read the data\n",
        "\n",
        "First, we'll download the file and then decode."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pD_55cOxLkAb",
        "tags": []
      },
      "outputs": [],
      "source": [
        "path_to_file = tf.keras.utils.get_file(\n",
        "    \"shakespeare.txt\",\n",
        "    \"https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\",\n",
        ")\n",
        "#tf.keras.utils.get_file() 函数会自动下载指定 URL 的文件，并将其保存到本地。\n",
        "#如果文件已经存在，它将不会重新下载，而是返回文件的路径。如果文件不存在，它会下载文件并返回保存的路径。"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**一般来说，在大多数 Unix/Linux 系统上，文件会被保存在 ~/.keras/datasets/ 目录下。而在 Windows 系统上，文件会被保存在 C:\\Users\\{username}\\.keras\\datasets\\ 目录下。**"
      ],
      "metadata": {
        "id": "M0xoyPTbjRUC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aavnuByVymwK",
        "tags": [],
        "outputId": "c6abf5a7-2d4e-4bf5-ab2d-028c70ad9e49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ],
      "source": [
        "text = open(path_to_file, \"rb\").read().decode(encoding=\"utf-8\")\n",
        "# 打开之前下载的文件，以二进制模式读取内容 并按照UTF-8编码进行解码\n",
        "# 转换为Unicode字符串\n",
        "print(f\"Length of text: {len(text)} characters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uu161EGgg-rm"
      },
      "source": [
        "Let's take a look at the first 250 characters in text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Duhg9NrUymwO",
        "tags": [],
        "outputId": "ee79985c-9905-4aa4-8f99-9c0f6f5ee952",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(text[:250])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-jLYYOVg-rm"
      },
      "source": [
        "Let's check to see how many unique characters are in our corpus/document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlCgQBRVymwR",
        "tags": [],
        "outputId": "031d403d-80b2-4301-9c87-869fa2fc4352",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 unique characters\n"
          ]
        }
      ],
      "source": [
        "vocab = sorted(set(text))\n",
        "# set(text)创建了一个包含文本中所有字符的集合 集合的特性是其中的元素是唯一的\n",
        "# sorted() 排序并返回\n",
        "print(f\"{len(vocab)} unique characters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNnrKn_lL-IJ"
      },
      "source": [
        "## Process the text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a86OoYtO01go",
        "tags": [],
        "outputId": "ee792952-60ce-4306-fddd-589d622c8c2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "example_texts = [\"abcdefg\", \"xyz\"]\n",
        "\n",
        "# TODO 1\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding=\"UTF-8\")\n",
        "chars"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1s4f1q3iqY8f"
      },
      "source": [
        "Now create the `tf.keras.layers.StringLookup` layer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GMlCe3qzaL9",
        "tags": []
      },
      "outputs": [],
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None\n",
        ")\n",
        "# 首先，它创建了一个 tf.keras.layers.StringLookup 层的实例，命名为 ids_from_chars。\n",
        "\n",
        "# 这个实例被配置为使用提供的词汇表（vocabulary=list(vocab)），这个词汇表包含了文本中所有唯一的字符，因此这个实例将能够将这些字符映射成唯一的数字 ID。\n",
        "\n",
        "# 配置中设置了 mask_token=None，表示在处理输入时不使用遮罩标记。\n",
        "\n",
        "# 这个 ids_from_chars 实例现在可以接受字符作为输入，并将其转换为数字 ID。这个过程是通过调用这个实例来实现的，例如 ids_from_chars(\"a\") 将返回字符 \"a\" 对应的数字 ID。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFjSVAlWzf-N"
      },
      "source": [
        "### Vectorize the text\n",
        "\n",
        "Before training, you need to convert the strings to a numerical representation.\n",
        "\n",
        "Using `tf.keras.layers.StringLookup` layer can convert each character into a numeric ID. It just needs the text to be split into tokens first.\n",
        "\n",
        "\n",
        "神经网络无法直接处理文本数据，所以一般需要先将输入数据表示为数值形式，进而计算和优化。\n",
        "\n",
        "1.分隔文本为标记或字符： 将文本分割成单个的标记或字符，然后作为输入。\n",
        "\n",
        "2.将字符映射成数学ID：使用‘tf.keras.layers.StringLookup’层，将每个字符映射成一个数字ID。这样就可以转换成神经网络可以处理的数值形式。\n",
        "\n",
        "3.构建输入序列：将每个字符的数字ID组成的序列作为模型的输入。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmX_jbgQqfOi"
      },
      "source": [
        "It converts from tokens to character IDs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLv5Q_2TC2pc",
        "tags": [],
        "outputId": "2b9bf66f-0f4a-4010-afea-69491f7fccd9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZfqhkYCymwX"
      },
      "source": [
        "Since the goal of this tutorial is to generate text, it will also be important to invert this representation and recover human-readable strings from it. For this you can use `tf.keras.layers.StringLookup(..., invert=True)`.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uenivzwqsDhp"
      },
      "source": [
        "Note: Here instead of passing the original vocabulary generated with `sorted(set(text))` use the `get_vocabulary()` method of the `tf.keras.layers.StringLookup` layer so that the `[UNK]` tokens is set the same way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wd2m3mqkDjRj",
        "tags": []
      },
      "outputs": [],
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqTDDxS-s-H8"
      },
      "source": [
        "This layer recovers the characters from the vectors of IDs, and returns them as a `tf.RaggedTensor` of characters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2GCh0ySD44s",
        "tags": [],
        "outputId": "791eccfb-2e3c-4293-d186-3933b5d4875e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "chars = chars_from_ids(ids)\n",
        "chars"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FeW5gqutT3o"
      },
      "source": [
        "You can `tf.strings.reduce_join` to join the characters back into strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxYI-PeltqKP",
        "tags": [],
        "outputId": "09658c1f-fb14-457b-d440-b859183a2829",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5apvBDn9Ind",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def text_from_ids(ids):\n",
        "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "上面相当于既讲了如何将字符对应成数字，也讲了如何逆转重新返回字符串"
      ],
      "metadata": {
        "id": "ra-ir5Puo8TQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbmsf23Bymwe"
      },
      "source": [
        "### The prediction task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wssHQ1oGymwe"
      },
      "source": [
        "Given a character, or a sequence of characters, what is the most probable next character? This is the task you're training the model to perform. The input to the model will be a sequence of characters, and you train the model to predict the output—the following character at each time step.\n",
        "\n",
        "Since RNNs maintain an internal state that depends on the previously seen elements, given all the characters computed until this moment, what is the next character?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RNN（循环神经网络）具有记忆性，它会维护一个内部状态，这个内容状态依赖于之前观察到的元素。\n",
        "\n",
        "这个任务被称为字符级别的文本生成（character-level text generation）。在训练过程中，模型会学习文本序列中的模式和语义信息，并试图预测下一个字符。\n",
        "\n",
        "在预测过程中，模型使用前一个时间步的输出和隐藏状态来生成下一个时间步的字符，从而生成连续的文本序列。"
      ],
      "metadata": {
        "id": "mQam6C3wsopT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgsVvVxnymwf"
      },
      "source": [
        "### Create training examples and targets\n",
        "\n",
        "Next divide the text into example sequences. Each input sequence will contain `seq_length` characters from the text.\n",
        "\n",
        "For each input sequence, the corresponding targets contain the same length of text, except shifted one character to the right.\n",
        "\n",
        "So break the text into chunks of `seq_length+1`. For example, say `seq_length` is 4 and our text is \"Hello\". The input sequence would be \"Hell\", and the target sequence \"ello\".\n",
        "\n",
        "First use the `tf.data.Dataset.from_tensor_slices` function to convert the text vector into a stream of character indices."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "我们将文本划分为长度为 seq_length + 1 的序列，这样每个序列中的前 seq_length 个字符就是输入序列，而最后一个字符则是目标序列中的第一个字符。这样做的好处是可以很方便地从文本中提取输入序列和目标序列，并将它们用于模型的训练。同时，由于输入序列和目标序列的长度相同，这种划分方式也有助于保持序列数据的对齐性。"
      ],
      "metadata": {
        "id": "JHh91yQbt93I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UopbsKi88tm5",
        "tags": [],
        "outputId": "6d27e2e6-1f2d-481b-cad3-607e2fd1bcd2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "# TODO 2\n",
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, \"UTF-8\"))\n",
        "all_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmxrYDCTy-eL",
        "tags": []
      },
      "outputs": [],
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjH5v45-yqqH",
        "tags": [],
        "outputId": "4a188827-1a91-4add-ce48-a76db3d1041b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ],
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode(\"utf-8\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-G2oaTxy6km",
        "tags": []
      },
      "outputs": [],
      "source": [
        "seq_length = 100\n",
        "examples_per_epoch = len(text) // (seq_length + 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZSYAcQV8OGP"
      },
      "source": [
        "The `batch` method lets you easily convert these individual characters to sequences of the desired size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpdjRO2CzOfZ",
        "tags": [],
        "outputId": "172b983f-f780-41ea-a5c7-acf2203abbb9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "sequences = ids_dataset.batch(seq_length + 1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "    print(chars_from_ids(seq))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PHW902-4oZt"
      },
      "source": [
        "It's easier to see what this is doing if you join the tokens back into strings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QO32cMWu4a06",
        "tags": [],
        "outputId": "c4dee491-bd1f-44fe-a571-00a93c4dd95e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ],
      "source": [
        "for seq in sequences.take(5):\n",
        "    print(text_from_ids(seq).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbLcIPBj_mWZ"
      },
      "source": [
        "For training you'll need a dataset of `(input, label)` pairs. Where `input` and\n",
        "`label` are sequences. At each time step the input is the current character and the label is the next character.\n",
        "\n",
        "Here's a function that takes a sequence as input, duplicates, and shifts it to align the input and label for each timestep:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NGu-FkO_kYU",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1] #将序列中除了最后一个字符外所有字符作为输入序列\n",
        "    target_text = sequence[1:] #除了第一个字符外都作为目标序列\n",
        "    return input_text, target_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxbDTJTw5u_P",
        "tags": [],
        "outputId": "fa9a1127-cf66-4cc6-8262-a44dacc05662",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ],
      "source": [
        "split_input_target(list(\"Tensorflow\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9iKPXkw5xwa",
        "tags": []
      },
      "outputs": [],
      "source": [
        "dataset = sequences.map(split_input_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNbw-iR0ymwj",
        "tags": [],
        "outputId": "531b2390-0535-48bd-f217-568de2255b39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ],
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJdfPmdqzf-R"
      },
      "source": [
        "### Create training batches\n",
        "\n",
        "You used `tf.data` to split the text into manageable sequences. But before feeding this data into the model, you need to shuffle the data and pack it into batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2pGotuNzf-S",
        "tags": [],
        "outputId": "706e74b2-b35f-4fc7-c17b-fa634296a0ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ],
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64 #决定了每个批次的数据大小\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000 #决定了一次洗牌的数据量\n",
        "\n",
        "dataset = (\n",
        "    dataset.shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
        "    #洗牌 然后再分批次 并丢弃剩余的序列 以确保每个批次大小一致\n",
        ")\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6oUuElIMgVx"
      },
      "source": [
        "## Build The Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8gPwEjRzf-Z"
      },
      "source": [
        "This section defines the model as a `keras.Model` subclass (For details see [Making new Layers and Models via subclassing](https://www.tensorflow.org/guide/keras/custom_layers_and_models)).\n",
        "\n",
        "#### TODO 3 Build a model with the following layers\n",
        "\n",
        "* `tf.keras.layers.Embedding`: The input layer. A trainable lookup table that will map each character-ID to a vector with `embedding_dim` dimensions;\n",
        "* `tf.keras.layers.GRU`: A type of RNN with size `units=rnn_units` (You can also use an LSTM layer here.)\n",
        "* `tf.keras.layers.Dense`: The output layer, with `vocab_size` outputs. It outputs one logit for each character in the vocabulary. These are the log-likelihood of each character according to the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHT8cLh7EAsg",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KS8yqELDg-rr"
      },
      "source": [
        "The class below does the following:\n",
        "- We derive a class from tf.keras.Model\n",
        "- The constructor is used to define the layers of the model\n",
        "- We define the pass forward using the layers defined in the constructor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wj8HQ2w8z4iO",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "        super().__init__(self)\n",
        "        # TODO - Create an embedding layer\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        # TODO - Create a GRU layer\n",
        "        self.gru = tf.keras.layers.GRU(\n",
        "            rnn_units, return_sequences=True, return_state=True\n",
        "        )\n",
        "        # TODO - Finally connect it with a dense layer\n",
        "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, inputs, states=None, return_state=False, training=False):\n",
        "        x = self.embedding(inputs, training=training)\n",
        "        # since we are training a text generation model,\n",
        "        # we use the previous state, in training. If there is no state,\n",
        "        # then we initialize the state\n",
        "        if states is None:\n",
        "            states = self.gru.get_initial_state(x)\n",
        "        x, states = self.gru(x, initial_state=states, training=training)\n",
        "        x = self.dense(x, training=training)\n",
        "\n",
        "        if return_state:\n",
        "            return x, states\n",
        "        else:\n",
        "            return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "在 __init__ 方法中，首先通过 tf.keras.layers.Embedding 创建了一个嵌入层 self.embedding，用于将字符的数字 ID 映射成指定维度的嵌入向量。嵌入向量的维度由 embedding_dim 参数指定。\n",
        "\n",
        "接着，通过 tf.keras.layers.GRU 创建了一个 GRU 层 self.gru，用于处理序列数据。GRU 层的参数包括 rnn_units 指定了 GRU 单元的数量，以及 return_sequences=True, return_state=True 指定了返回序列数据和隐藏状态。\n",
        "\n",
        "最后，通过 tf.keras.layers.Dense 创建了一个全连接层 self.dense，该层的输出维度与词汇表大小 vocab_size 相同，用于生成每个字符的概率分布。\n",
        "\n",
        "在 call 方法中，定义了模型的前向传播过程。首先，输入数据经过嵌入层 self.embedding 进行嵌入表示，然后传入 GRU 层 self.gru 进行序列建模，最后经过全连接层 self.dense 输出模型的预测结果。\n",
        "\n",
        "在训练阶段，模型可以接收前一个时刻的隐藏状态，并将其作为 GRU 层的初始状态传入。如果没有提供隐藏状态，则通过调用 self.gru.get_initial_state(x) 来初始化隐藏状态。\n",
        "\n",
        "最后，根据是否需要返回隐藏状态，决定是否返回预测结果和隐藏状态。"
      ],
      "metadata": {
        "id": "-aldwluOruEv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IX58Xj9z47Aw",
        "tags": []
      },
      "outputs": [],
      "source": [
        "model = MyModel(\n",
        "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkA5upJIJ7W7"
      },
      "source": [
        "For each character the model looks up the embedding, runs the GRU one timestep with the embedding as input, and applies the dense layer to generate logits predicting the log-likelihood of the next character."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ubPo0_9Prjb"
      },
      "source": [
        "## Try the model\n",
        "\n",
        "Now run the model to see that it behaves as expected.\n",
        "\n",
        "First check the shape of the output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-_70kKAPrPU",
        "tags": [],
        "outputId": "fa922cbb-e7e1-4064-e9c8-d347db75d5a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ],
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(\n",
        "        example_batch_predictions.shape,\n",
        "        \"# (batch_size, sequence_length, vocab_size)\",\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6NzLBi4VM4o"
      },
      "source": [
        "In the above example the sequence length of the input is `100` but the model can be run on inputs of any length:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPGmAAXmVLGC",
        "tags": [],
        "outputId": "5366ef8d-a941-4157-e244-553dc8a96ccc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  16896     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4022850 (15.35 MB)\n",
            "Trainable params: 4022850 (15.35 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwv0gEkURfx1"
      },
      "source": [
        "To get actual predictions from the model you need to sample from the output distribution, to get actual character indices. This distribution is defined by the logits over the character vocabulary.\n",
        "\n",
        "Note: It is important to _sample_ from this distribution as taking the _argmax_ of the distribution can easily get the model stuck in a loop.\n",
        "\n",
        "Try it for the first example in the batch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4V4MfFg0RQJg",
        "tags": []
      },
      "outputs": [],
      "source": [
        "sampled_indices = tf.random.categorical(\n",
        "    #这个函数用于从概率分布中采样出指定数量的样本\n",
        "    example_batch_predictions[0], num_samples=1\n",
        "    #上面左边这项是一个表示模型对某个批次数据的预测结果的张量。\n",
        "    #通常形状是（batch_size,vocab_size)\n",
        "    #右边这项指定了需要从概率分布中采样的样本数量，设置为1，只要概率最大的字符索引\n",
        ")\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
        "# 将采样得到的样本索引压缩成一维张量，转换为Numpy数组"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QM1Vbxs_URw5"
      },
      "source": [
        "This gives us, at each timestep, a prediction of the next character index:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqFMUQc_UFgM",
        "tags": [],
        "outputId": "528b509f-20ed-414b-9372-1c1a544a2332",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([29, 19,  9, 51, 13, 26, 11, 63, 32, 64, 39, 17, 44, 61, 43, 23, 40,\n",
              "       35, 55, 38, 14, 64, 24, 56, 23, 62,  7, 52, 42, 34, 32, 20, 19, 47,\n",
              "       34, 38, 28, 17, 61, 64, 23, 46, 64, 58, 34, 62, 29, 17, 23, 39, 26,\n",
              "        8,  5, 22, 29,  2, 25, 52, 37,  0, 40, 49, 23, 56, 41, 24,  2,  2,\n",
              "       18, 11, 62, 23, 17, 64, 52, 55, 51, 10,  6, 53, 37, 50, 12, 60, 54,\n",
              "        1, 21, 65, 27, 52, 53, 58, 20, 52, 19, 62, 42, 24, 62, 44])"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ],
      "source": [
        "sampled_indices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfLtsP3mUhCG"
      },
      "source": [
        "Decode these to see the text predicted by this untrained model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWcFwPwLSo05",
        "tags": [],
        "outputId": "c64edca9-1644-4fe5-9e34-ff2df2f3f487",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b\"n\\nSigh'd truer breath; but that I see thee here,\\nThou noble thing! more dances my rapt heart\\nThan wh\"\n",
            "\n",
            "Next Char Predictions:\n",
            " b\"PF.l?M:xSyZDevdJaVpYAyKqJw,mcUSGFhUYODvyJgysUwPDJZM-&IP LmX[UNK]ajJqbK  E:wJDympl3'nXk;uo\\nHzNmnsGmFwcKwe\"\n"
          ]
        }
      ],
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJL0Q0YPY6Ee"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCbHQHiaa4Ic"
      },
      "source": [
        "At this point the problem can be treated as a standard classification problem. Given the previous RNN state, and the input this time step, predict the class of the next character."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAjbjY03eiQ4"
      },
      "source": [
        "#### Attach an optimizer, and a loss function\n",
        "The standard `tf.keras.losses.sparse_categorical_crossentropy` loss function works in this case because it is applied across the last dimension of the predictions.\n",
        "\n",
        "Because your model returns logits, you need to set the `from_logits` flag.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "当模型返回的是 logit 值时，需要设置 from_logits 参数。Logit 是指未经过 softmax 函数处理的模型输出值。在使用交叉熵损失函数计算损失时，如果模型输出的是 logit 值，通常需要将其设置为 from_logits=True。\n",
        "\n",
        "设置 from_logits=True 表示损失函数会自动对模型输出的 logit 值应用 softmax 函数，将其转换为概率分布，然后再计算交叉熵损失。这样可以确保损失函数计算的是正确的损失值，并且与模型输出的数据类型一致。"
      ],
      "metadata": {
        "id": "_1Xr7O0d0QlD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZOeWdgxNFDXq",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# TODO - add a loss function here\n",
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HrXTACTdzY-",
        "tags": [],
        "outputId": "a12f3486-f220-4227-d767-9ba7886b1bd9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.1909094, shape=(), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\n",
        "    \"Prediction shape: \",\n",
        "    example_batch_predictions.shape,\n",
        "    \" # (batch_size, sequence_length, vocab_size)\",\n",
        ")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkvUIneTFiow"
      },
      "source": [
        "A newly initialized model shouldn't be too sure of itself, the output logits should all have similar magnitudes. To confirm this you can check that the exponential of the mean loss is approximately equal to the vocabulary size. A much higher loss means the model is sure of its wrong answers, and is badly initialized:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "这段描述指的是在初始化模型时，模型对于输出的预测应该具有相似的数量级，而不应该出现某些预测值特别大或特别小的情况。具体来说，对于分类任务而言，模型在初始状态下对于每个类别的预测概率应该是相对平均的，而不是对某个类别的预测概率特别自信。\n",
        "\n",
        "为了确认模型是否被正确初始化，可以计算模型的损失值，并检查损失值的平均值的指数函数是否接近于词汇表的大小。如果损失值较高，这意味着模型的预测偏离了正确答案，模型可能被错误地初始化了"
      ],
      "metadata": {
        "id": "Pend6aVu1gJz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAJfS5YoFiHf",
        "tags": [],
        "outputId": "d54da9f3-4190-4a60-8361-f25fbbbf3c91",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "66.082855"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ],
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeOXriLcymww"
      },
      "source": [
        "Configure the training procedure using the `tf.keras.Model.compile` method. Use `tf.keras.optimizers.Adam` with default arguments and the loss function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDl1_Een6rL0",
        "tags": []
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=\"adam\", loss=loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieSJdchZggUj"
      },
      "source": [
        "### Configure checkpoints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6XBUUavgF56"
      },
      "source": [
        "Use a `tf.keras.callbacks.ModelCheckpoint` to ensure that checkpoints are saved during training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6fWTriUZP-n",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = \"./training_checkpoints\"\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix, save_weights_only=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ky3F_BhgkTW"
      },
      "source": [
        "### Execute the training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxdOA-rgyGvs"
      },
      "source": [
        "To keep training time reasonable, use 10 epochs to train the model. In Colab, set the runtime to GPU for faster training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yGBE2zxMMHs",
        "tags": []
      },
      "outputs": [],
      "source": [
        "EPOCHS = 30 #10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UK-hmKjYVoll",
        "tags": [],
        "outputId": "f37122a6-094d-4e9c-e8c3-90ae53b78fc9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "172/172 [==============================] - 13s 62ms/step - loss: 1.1616\n",
            "Epoch 2/30\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 1.1216\n",
            "Epoch 3/30\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 1.0786\n",
            "Epoch 4/30\n",
            "172/172 [==============================] - 13s 58ms/step - loss: 1.0329\n",
            "Epoch 5/30\n",
            "172/172 [==============================] - 13s 60ms/step - loss: 0.9834\n",
            "Epoch 6/30\n",
            "172/172 [==============================] - 13s 61ms/step - loss: 0.9320\n",
            "Epoch 7/30\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 0.8805\n",
            "Epoch 8/30\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 0.8273\n",
            "Epoch 9/30\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 0.7779\n",
            "Epoch 10/30\n",
            "172/172 [==============================] - 12s 62ms/step - loss: 0.7280\n",
            "Epoch 11/30\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 0.6812\n",
            "Epoch 12/30\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 0.6418\n",
            "Epoch 13/30\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 0.6076\n",
            "Epoch 14/30\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 0.5757\n",
            "Epoch 15/30\n",
            "172/172 [==============================] - 12s 60ms/step - loss: 0.5490\n",
            "Epoch 16/30\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 0.5282\n",
            "Epoch 17/30\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 0.5131\n",
            "Epoch 18/30\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 0.4959\n",
            "Epoch 19/30\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 0.4823\n",
            "Epoch 20/30\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 0.4712\n",
            "Epoch 21/30\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 0.4603\n",
            "Epoch 22/30\n",
            "172/172 [==============================] - 12s 57ms/step - loss: 0.4527\n",
            "Epoch 23/30\n",
            "172/172 [==============================] - 13s 60ms/step - loss: 0.4488\n",
            "Epoch 24/30\n",
            "172/172 [==============================] - 13s 62ms/step - loss: 0.4446\n",
            "Epoch 25/30\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 0.4407\n",
            "Epoch 26/30\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 0.4341\n",
            "Epoch 27/30\n",
            "172/172 [==============================] - 12s 60ms/step - loss: 0.4332\n",
            "Epoch 28/30\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 0.4288\n",
            "Epoch 29/30\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 0.4329\n",
            "Epoch 30/30\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 0.4265\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKkD5M6eoSiN"
      },
      "source": [
        "#### Generate text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIdQ8c8NvMzV"
      },
      "source": [
        "The simplest way to generate text with this model is to run it in a loop, and keep track of the model's internal state as you execute it.\n",
        "\n",
        "Each time you call the model you pass in some text and an internal state. The model returns a prediction for the next character and its new state. Pass the prediction and state back in to continue generating text.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjGz1tDkzf-u"
      },
      "source": [
        "The following makes a single step prediction:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSBU1tHmlUSs",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "    def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "        self.model = model\n",
        "        self.chars_from_ids = chars_from_ids\n",
        "        self.ids_from_chars = ids_from_chars\n",
        "\n",
        "        # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "        skip_ids = self.ids_from_chars([\"[UNK]\"])[:, None]\n",
        "        sparse_mask = tf.SparseTensor(\n",
        "            # Put a -inf at each bad index.\n",
        "            values=[-float(\"inf\")] * len(skip_ids),\n",
        "            indices=skip_ids,\n",
        "            # Match the shape to the vocabulary\n",
        "            dense_shape=[len(ids_from_chars.get_vocabulary())],\n",
        "        )\n",
        "        self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "    @tf.function\n",
        "    def generate_one_step(self, inputs, states=None):\n",
        "        # Convert strings to token IDs.\n",
        "        input_chars = tf.strings.unicode_split(inputs, \"UTF-8\")\n",
        "        input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "        # Run the model.\n",
        "        # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "        predicted_logits, states = self.model(\n",
        "            inputs=input_ids, states=states, return_state=True\n",
        "        )\n",
        "        # Only use the last prediction.\n",
        "        predicted_logits = predicted_logits[:, -1, :]\n",
        "        predicted_logits = predicted_logits / self.temperature\n",
        "        # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "        predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "        # Sample the output logits to generate token IDs.\n",
        "        predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "        predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "        # Convert from token ids to characters\n",
        "        predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "        # Return the characters and model state.\n",
        "        return predicted_chars, states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqMOuDutnOxK",
        "tags": []
      },
      "outputs": [],
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9yDoa0G3IgQ"
      },
      "source": [
        "Run it in a loop to generate some text. Looking at the generated text, you'll see the model knows when to capitalize, make paragraphs and imitates a Shakespeare-like writing vocabulary. With the small number of training epochs, it has not yet learned to form coherent sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ST7PSyk9t1mT",
        "tags": [],
        "outputId": "da13b6af-bf7e-4752-9579-9747c3656400",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "There is no end, no sight,\n",
            "Fet in their affections waves light of his back,\n",
            "Both whose chois with Barnardone and in France,\n",
            "And all my life by death!' I am afraid.\n",
            "\n",
            "PARIS:\n",
            "More with that I should were the rid of that word:\n",
            "Sir, crying my gage, that now report\n",
            "And scatter'd them the charget? I would\n",
            "the duke is dead; and the woef is dangerous there:\n",
            "Or, if she be so ready, and not unknown behold\n",
            "And death with some mermior, all of precenting,\n",
            "Which I would have liars himself, as it a pleasant gentleman,\n",
            "Which follows in my dust. My royal hands,\n",
            "Thy beam of light strange the nobles.\n",
            "But let his met a little full of worship,\n",
            "Doable success, I have touch'd with the heart,\n",
            "To think thou spentted thirty minister\n",
            "Upon a care from Clifford's life:--fligh, thou art a\n",
            "cold opinion, whether you dinded it?\n",
            "\n",
            "GLOUCESTER:\n",
            "I cry thee to him light.\n",
            "\n",
            "MERCUTIO:\n",
            "Tut, I'll swear, where's the world to bed wife.\n",
            "Some most noble visit have I lain with you.\n",
            "\n",
            "SEBASTIAN:\n",
            "Why, ay, for I have.\n",
            "\n",
            "First Murderer:\n",
            "Ri \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.2944185733795166\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant([\"ROMEO:\"])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "    next_char, states = one_step_model.generate_one_step(\n",
        "        next_char, states=states\n",
        "    )\n",
        "    result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode(\"utf-8\"), \"\\n\\n\" + \"_\" * 80)\n",
        "print(\"\\nRun time:\", end - start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AM2Uma_-yVIq"
      },
      "source": [
        "The easiest thing you can do to improve the results is to train it for longer (try `EPOCHS = 30`).\n",
        "\n",
        "You can also experiment with a different start string, try adding another RNN layer to improve the model's accuracy, or adjust the temperature parameter to generate more or less random predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OfbI4aULmuj"
      },
      "source": [
        "If you want the model to generate text *faster* the easiest thing you can do is batch the text generation. In the example below the model generates 5 outputs in about the same time it took to generate 1 above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZkLu7Y8UCMT7",
        "tags": [],
        "outputId": "1f0a1d9b-fa7a-4daa-c6c1-9b9fc35c8051",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b\"ROMEO:\\nIt id me hell.\\n\\nGONZALO:\\nSir, were you are.\\n\\nDUKE VINCENTIO:\\nThat thou hast vengeance for the dead?\\n\\nSecond Servingman:\\n'Tis thee!\\n\\nLADY CAPULET:\\nHe megles me now, by our breaths, disbrince thou\\ndost thou and myself a banish up.\\n\\nSecond Citizen:\\nBe abroad in the sacred: as I underst\\nand affection of his life'd; come noble strange\\nThat I have, ensured.\\nI love them; and I, with their enemy,\\nFill thy trembling show, is well regreet\\nMy tongue that they dream.\\n\\nDUKE OF AUMERLE:\\nSo do I what no man in the blocks be bride?\\nThey kiss this intend tongue than they did burns.\\n\\nTRANIO:\\nA haveness' loving sensible Master\\nForbids the trade. IV\\nAughblowly, when ground dignitive and Bolibba\\nMy brother Clarence, peefered? My friends, are edgar\\nNay: give what I we good friend.\\n\\nAETOLINAB:\\nI'll be goddes ig spirits\\nTo heal thee trustmy Edward's miscree is here:\\nI prithee not for hither ratee.\\n\\nDUCHESS OF YORK:\\nWhat time this worthy?\\n\\nSecond Capultis Monoo,\\nI have not their worship of your breast,\\nWill d\"\n",
            " b\"ROMEO:\\nHe has not Edward and as backness, shall requite due.\\n\\nKING LEWIS XI:\\nWife, by Third it be into that Poverers;\\nOr else by the partice sleep and IWalk,\\nContilict with his terring throne in this knees,\\nAlless that have timing flowers have almost\\nBetor him too qualif years, if you be mort\\nof it. Now, given this time thou wert despite\\nWhen stecter'd strength could in Peter's flight\\nAnd Capulet at revenge to the wall:\\nThe king shall find out a pinchumbery, kings,\\nTurn thousand person' through the throne to France,\\nAnd stayly perchieved up his king-bless into the truth.\\n\\nGLOUCESTER:\\nHe hath alre hath done with thee:\\nInfair supplused sister strift on wear.\\n\\nBIANCA:\\nTears him fairted toward the hand of love.\\n\\nRICHARD:\\nPardon will it not judge me, Marcius, Rivers,\\nAll hourly last gold gentleman-stides,\\nNed with sorrow begend it in,\\nAnd speak me up before Frovouring doth ang regrant.\\n\\nKING HENRY VI:\\nSwell'd yield a loss, babour withal.\\n\\nKING RICHARD III:\\nI veave your danger to their purseive\\nHa\"\n",
            " b\"ROMEO:\\nAll this we great what's this? Core to your judgment town?\\n\\nBUCKINGHAM:\\nYou may not be? Exeter, puck thy such bold.\\n\\nSICINIUS:\\nNot so hap but hither down too?\\n\\nJULIET:\\nO Remerves and the dissonue must they do.\\nWas netting lattless vows wrong'd, I will bet\\ndeep none of that way I am look'd upon this enemie,\\nYour cold bawding, I did; crap'd not to be well: nay, my good\\nCan be but hearing: if they repent\\nFrom this world live accursed with behavoid.\\nSirt, this is betatian's eegards be not: Norfolk,\\nThe frimate brother amports thee,\\nWhen it yet gentlewomen and ombtions.\\n\\nELBOW:\\nI promised.\\n\\nKING RICHARD II:\\nWe will: our framed men will beseech your brooder,\\nWhen therein let me enter\\nto and the monument and heaven.\\n\\nBRUTUS:\\nBe perform.\\n\\nJULIET:\\nWhere is the noble magge\\nHad I kept with myself? for he is liberty\\nToo lie the freedin of other,\\nAnd what you have nought's externily's flint,\\nBut very princels may not to have\\nAccuss and create thy father in his pawn't;\\nFull for sorrows great Baptis\"\n",
            " b\"ROMEO:\\nThe botch of the weary beautes when Tarious Richmond,\\nTo my strengths, the law what news by Sake from my friends,\\nAnd when he is, the king may been sleep,\\nBreak it, and as westly, my is not hand but a shroubly.\\n\\nYORK:\\nTherefore, to have the proviged cut bids\\nOut of honourable stingings, that\\nIs appideous kind offence, wasicing a\\nwhat I see, that fair supplisied arm;\\nIf our mooning doth he has himself.\\n\\nPRISPERO:\\nThis is it?\\n\\nDUKE VINCENTIO:\\nNow, fairh, sir, if they see' and leave thee well,\\nAnd watch thy thront their abroad twenty digoces;\\nFrom such a pleasure weeps at nutsy,\\nThough from came to the English king.\\n\\nGEORT:\\nWell believe thy most will in our brother Richard.\\n\\nKATHARINA:\\nI darny offence.\\n\\nMARIANA:\\nI fear now, what's wingman, no ave\\nSince the wail quickly set their missemness:\\nThou abter's pope, an quickly moon break'd as you came\\nTo rise the worthy offence of your lives.\\n\\nCOMINIUS:\\nYou Know: most life, with hastes was I know,\\nIt was this, I did even thy name\\nShould boy, th\"\n",
            " b\"ROMEO:\\nThen feather makely want not. How lad,\\nhaving no upon: you are one that gives me heaven\\nMy brother't like men his daughter:\\nMethinks I see, is't die withal.\\n\\nKING RICHARD III:\\nThat' that last good to hide: this, madam,\\nIs not might friendly lady to Englishmen;\\nFear better what they do have person?\\n\\nFirst Soldier:\\nI would look you to his counterfrign.\\n\\nSICINIUS:\\nReady, let him nobly sleed there's\\nsimpledge: asise me nothing:\\nInfertatter'd me to armour Mighton;\\nWhere Even my occupation howling, tell me,\\nMy brother could nature mouth, and Edward\\nThey're behelds; if else himself have\\nO villany, no more.\\n\\nDUKE OF YORK:\\nMy gracious ladge, lie, till I call thee:\\nThus, madam, yes,--that true manners let the messeg art\\nWas no mourner.\\n\\nPETRUCHIO:\\nAy, but I have to visit me:\\nIf you have chosen so quechion, yea,\\nThe gest we gladly gentlemen of their\\nart and the place of Edward's brows! Buy! 'Rome,\\nOr we are blooding dine to make Edward's face\\nTying, and Sulfam labour ere I, nor\\nThat with a'siric\"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 4.73727822303772\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant([\"ROMEO:\", \"ROMEO:\", \"ROMEO:\", \"ROMEO:\", \"ROMEO:\"])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "    next_char, states = one_step_model.generate_one_step(\n",
        "        next_char, states=states\n",
        "    )\n",
        "    result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, \"\\n\\n\" + \"_\" * 80)\n",
        "print(\"\\nRun time:\", end - start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlUQzwu6EXam"
      },
      "source": [
        "## Export the generator\n",
        "\n",
        "This single-step model can easily be [saved and restored](https://www.tensorflow.org/guide/saved_model), allowing you to use it anywhere a `tf.saved_model` is accepted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Grk32H_CzsC",
        "tags": []
      },
      "outputs": [],
      "source": [
        "tf.saved_model.save(one_step_model, \"one_step\")\n",
        "one_step_reloaded = tf.saved_model.load(\"one_step\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Z9bb_wX6Uuu",
        "tags": []
      },
      "outputs": [],
      "source": [
        "states = None\n",
        "next_char = tf.constant([\"ROMEO:\"])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "    next_char, states = one_step_reloaded.generate_one_step(\n",
        "        next_char, states=states\n",
        "    )\n",
        "    result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4QwTjAM6A2O"
      },
      "source": [
        "## Advanced: Customized Training\n",
        "\n",
        "The above training procedure is simple, but does not give you much control.\n",
        "It uses teacher-forcing which prevents bad predictions from being fed back to the model, so the model never learns to recover from mistakes.\n",
        "\n",
        "So now that you've seen how to run the model manually next you'll implement the training loop. This gives a starting point if, for example, you want to implement _curriculum  learning_ to help stabilize the model's open-loop output.\n",
        "\n",
        "The most important part of a custom training loop is the train step function.\n",
        "\n",
        "Use `tf.GradientTape` to track the gradients. You can learn more about this approach by reading the [eager execution guide](https://www.tensorflow.org/guide/eager).\n",
        "\n",
        "The basic procedure is:\n",
        "\n",
        "1. Execute the model and calculate the loss under a `tf.GradientTape`.\n",
        "2. Calculate the updates and apply them to the model using the optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0pZ101hjwW0",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class CustomTraining(MyModel):\n",
        "    @tf.function\n",
        "    def train_step(self, inputs):\n",
        "        inputs, labels = inputs\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self(inputs, training=True)\n",
        "            loss = self.loss(labels, predictions)\n",
        "        grads = tape.gradient(loss, model.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "        return {\"loss\": loss}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Oc-eJALcK8B"
      },
      "source": [
        "The above implementation of the `train_step` method follows [Keras' `train_step` conventions](https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit). This is optional, but it allows you to change the behavior of the train step and still use keras' `Model.compile` and `Model.fit` methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKyWiZ_Lj7w5",
        "tags": []
      },
      "outputs": [],
      "source": [
        "model = CustomTraining(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U817KUm7knlm",
        "tags": []
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o694aoBPnEi9",
        "tags": []
      },
      "outputs": [],
      "source": [
        "model.fit(dataset, epochs=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8nAtKHVoInR"
      },
      "source": [
        "Or if you need more control, you can write your own complete custom training loop:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4tSNwymzf-q",
        "tags": []
      },
      "outputs": [],
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "mean = tf.metrics.Mean()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    mean.reset_states()\n",
        "    for batch_n, (inp, target) in enumerate(dataset):\n",
        "        logs = model.train_step([inp, target])\n",
        "        mean.update_state(logs[\"loss\"])\n",
        "\n",
        "        if batch_n % 50 == 0:\n",
        "            template = (\n",
        "                f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
        "            )\n",
        "            print(template)\n",
        "\n",
        "    # saving (checkpoint) the model every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "    print()\n",
        "    print(f\"Epoch {epoch+1} Loss: {mean.result().numpy():.4f}\")\n",
        "    print(f\"Time taken for 1 epoch {time.time() - start:.2f} sec\")\n",
        "    print(\"_\" * 80)\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "environment": {
      "kernel": "python3",
      "name": "tf2-gpu.2-12.m109",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-12:m109"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}